{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning Traffic Sign Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'data/train.p'\n",
    "validation_file='data/test.p'\n",
    "testing_file = 'data/valid.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "x_train, y_train = train['features'], train['labels']\n",
    "x_valid, y_valid = valid['features'], valid['labels']\n",
    "x_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n",
    "Complete the basic data summary below. Use python, numpy and/or pandas methods to calculate the data summary rather than hard coding the results. For example, the [pandas shape method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shape.html) might be useful for calculating some of the summary results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = x_train.shape[0]\n",
    "n_validation = x_valid.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "image_shape = x_train[0].shape\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are around 34000 images in our training data that display 43 seperate classes of traffic signs. The input comes as a 32x32 image that contains RGB channels.\n",
    "\n",
    "We will train the model to learn from the training set, we use the validation set to see the effect on a bigger scale when changing the hyperparameters and will ultimatly measure the models performance against the 4410 examples of the test set.\n",
    "\n",
    "A minimum of 0.93 validation accuracy will be required for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include an exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that can be seen in the data is that its ordered. To display a variaty of example pictures, I will slice the training data roughly with the average count each image should be included.  \n",
    "\n",
    "The road signs have some interesting properties, which might be interesting for processing later:\n",
    "\n",
    "Shapes:\n",
    "- mostly round shapes (75%)\n",
    "- triangular is 2nd highest but features up and down (eg. Traffic Light or Yield)\n",
    "- rectangle occurs once (Priority Road)\n",
    "- hexagon occurs once (Stop)\n",
    "\n",
    "Colors:\n",
    "- most dominatant colors are clearly red, blue and white\n",
    "- blue signs always feature one or more white arrows, which determine its class (also red signs dont feature white arrows since their background is white), therefore the blue channel does not hold much information\n",
    "- red signs come in multiple shapes, but the most similar signs are round with the information of the sign in the middle (eg Speed Limit, driving limitations or caution signals) \n",
    "\n",
    "General:\n",
    "- some images are really bright, some are almost black. The images will have to be normalized to clearly identify the distinct parts of each class\n",
    "- traffic sign and background are not clearly distinct, contrast needs to be normalized\n",
    "- RGB channels should be disgarded in favor of R channel, since blue signs show distinct properties anyway and the only yellow sign has a unique shape. Red channel is clearly the best option, since the majority of observations display red color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 5\n",
    "cols = 10\n",
    "f = int((x_train.shape[0] * 0.9) / len(set(y_train)))\n",
    "\n",
    "fig, ax = plt.subplots(rows,cols, dpi=160)\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(rows*cols):\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(x_train[i * 700], cmap='gray')    \n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 43 different classes, one thing that we have to consider when implementing a neural network is distribution bias. With the given distribution of pictures our model will favor higher occuring road signs when unsure what to pick. There is no ground truth here, but an equal occurance will to prevent our model from overfitting in a particular direction, wheras keeping the given distribution will favor more frequently occuring signs in real world. When checking the distributions of the images, we can clearly see that some images occur around 2000 times, whereas some class occurances are only around 200 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10), dpi=100)\n",
    "sns.countplot(ax=ax, data=pd.DataFrame(y_train),y=0, order=pd.DataFrame(y_train)[0].value_counts().index)\n",
    "ax.set(xlabel='Occurance', ylabel='Sign')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since we learned earlier that the train data has a distribution bias, I tried two approaches to achieve a better validation accuracy. Regardless of the bias, we need to create more training data by augmenting the given data set. When thinking of how these images will be taken from a vehicle, the augmenting operations will be:\n",
    "- blurring (to simulate a driving vehicle)\n",
    "- perspective transform (either left or right to simulate the perspective of taking the picture)\n",
    "- rotation (to improve stability of observations) \n",
    "\n",
    "The augmentations have some sort of randomizer in them, that determines the blur, the degree of rotation, and the warp to left or right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_img(img):\n",
    "    kernel = np.random.choice(np.array([3,5]))\n",
    "    return cv2.medianBlur(img, kernel)\n",
    "\n",
    "def rotate(img):\n",
    "    rot_degree = np.random.choice(np.array([10,15,20,25,30]))\n",
    "    rot_size = np.random.choice(np.array([0.8,0.9,1,1.1,1.2]))\n",
    "    rot_dir = np.random.choice(np.array([1, -1]))\n",
    "    rot = cv2.getRotationMatrix2D((16,16),rot_degree * rot_dir, rot_size)\n",
    "    return cv2.warpAffine(img, rot, (32,32))\n",
    "\n",
    "def warp(img):\n",
    "    \n",
    "    src = np.float32(\n",
    "    [[32,0], # bottom left\n",
    "     [32, 32], # bottom right\n",
    "     [0,0], # top left\n",
    "     [0, 32]]) # top right\n",
    "    \n",
    "    dst_l = np.float32(\n",
    "    [[32,5],\n",
    "     [32, 27],\n",
    "     [0,0],\n",
    "     [0, 32]])\n",
    "    dst_r = np.float32(\n",
    "    [[32,0],\n",
    "     [32, 32],\n",
    "     [0,5],\n",
    "     [0, 27]])    \n",
    "    \n",
    "    dst = dst_r if np.random.choice([0,1]) == 1 else dst_l\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    warped_image = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]), flags=cv2.INTER_LINEAR)\n",
    "    return warped_image\n",
    "\n",
    "def change_img(img):\n",
    "    opt = np.random.choice([0,1,2])\n",
    "    if opt == 0:\n",
    "        img = blur_img(img)\n",
    "    elif opt == 1:\n",
    "        img = rotate(img)\n",
    "    elif opt == 2:\n",
    "        img = warp(img)\n",
    "    return img\n",
    "\n",
    "cols = 10\n",
    "\n",
    "fig, ax = plt.subplots(1,cols, dpi=160, figsize=(5,5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(cols):\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(change_img(x_train[5000]), cmap='gray')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a short demonstration of the data augmentations, I state how many pictures of each class should be used for training. \n",
    "\n",
    "The two approaches feature creating a dictionary for each label and storing the indexes of the corresponding image within the key. Then I can either create a target sample amount (option 1) or to keep the distribution state how many times each image should be randomly augmented (option 2). This should in general improve the models performance on accuracy by more generalized approach to classification. \n",
    "\n",
    "After evaluation I achieved better results with Option 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {key:None for key in set(y_train)}\n",
    "\n",
    "for i in range(0,len(y_train)):\n",
    "    if data_dict[y_train[i]] == None:\n",
    "        data_dict[y_train[i]] = [i]\n",
    "    else:\n",
    "        data_dict[y_train[i]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 - Remove distribution bias\n",
    "\n",
    "target_samples = 3000\n",
    "\n",
    "new_samples = []\n",
    "\n",
    "# generate new samples\n",
    "for key in data_dict.keys():\n",
    "    samples_to_create = target_samples - len(data_dict[key])\n",
    "    for i in range(0, samples_to_create):\n",
    "        img = x_train[np.random.choice(data_dict[key])]\n",
    "        changed_img = change_img(img)\n",
    "        new_samples.append([\n",
    "            changed_img, key\n",
    "        ])  \n",
    "\n",
    "# merge with existing \n",
    "for i in range(0,len(x_train)):\n",
    "    new_samples.append([\n",
    "        x_train[i], y_train[i]\n",
    "    ])\n",
    "    \n",
    "random.shuffle(new_samples)\n",
    "len(new_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 - augment image n times (not used)\n",
    "\n",
    "#target_factor = 5\n",
    "\n",
    "#new_samples = []\n",
    "\n",
    "# generate new samples\n",
    "#for key in data_dict.keys():\n",
    "#    for sample in data_dict[key]:\n",
    "#        for i in range(target_factor):\n",
    "#            img = change_img(x_train[sample])\n",
    "#            new_samples.append([\n",
    "#                img, key\n",
    "#            ])\n",
    "    \n",
    "# merge with existing \n",
    "#for i in range(0,len(x_train)):\n",
    "#    new_samples.append([\n",
    "#        x_train[i], y_train[i]\n",
    "#    ])\n",
    "    \n",
    "#random.shuffle(new_samples)\n",
    "#len(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating augmented images of the originals, the input data is send through the preparation pipeline, consisting of:\n",
    "- contrast equalization to improve performance on bright or shady pictures\n",
    "- select only red channel\n",
    "- scale the values from 0,255 between 0,1\n",
    "\n",
    "The pipeline is then applied to train, validation and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(img, out_range=(0, 1), axis=None):\n",
    "    domain = np.min(img, axis), np.max(img, axis)\n",
    "    y = (img - (domain[1] + domain[0]) / 2) / (domain[1] - domain[0])\n",
    "    return y * (out_range[1] - out_range[0]) + (out_range[1] + out_range[0]) / 2\n",
    "\n",
    "def contrast_equalization(img):\n",
    "    img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "    img[:,:,1] = cv2.equalizeHist(img[:,:,1])\n",
    "    img[:,:,2] = cv2.equalizeHist(img[:,:,2])\n",
    "    return img\n",
    "\n",
    "\n",
    "for i in range(0,len(new_samples)):\n",
    "    new_samples[i][0] = contrast_equalization(new_samples[i][0])\n",
    "    new_samples[i][0] = scale(new_samples[i][0][:,:,0])\n",
    "    \n",
    "X_train = [x[0].reshape(32,32,1) for x in new_samples] \n",
    "y_train = [y[1] for y in new_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same has to be done with the validation / test data\n",
    "X_valid = [scale(contrast_equalization(i)[:,:,0]).reshape(32,32,1) for i in x_valid]\n",
    "X_test = [scale(contrast_equalization(i)[:,:,0]).reshape(32,32,1) for i in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The designed model is really similar to LeNet. It consists out of 3 Convolutional Layers and 4 flat Dense Layers. \n",
    "\n",
    "| Layer                                        | Shape    |\n",
    "|----------------------------------------------|----------|\n",
    "| Input Image                                  | 32x32x1  |\n",
    "| Conv2d / ReLu / Dropout (0.9)              | 28x28x8  |\n",
    "| Conv2d / ReLu / MaxPooling / Dropout (0.9) | 24x24x16 |\n",
    "| Conv2d / ReLu / MaxPooling / Dropout (0.9) | 20x20x32 |\n",
    "| Flatten / Dense / ReLu / Dropout (0.6)     | 512      |\n",
    "| Dense / ReLu / Dropout (0.6)               | 256      |\n",
    "| Dense / ReLu / Dropout (0.6)               | 128      |\n",
    "| Dense                                      | 43       |\n",
    "\n",
    "As for the hyperparameters, I choose 40 Epochs and a Batch Size of 256. \n",
    "\n",
    "With the later implemented decay (1e-5) of the learning rate (1e-3) I try to smooth the learning gradually over time to prevent the model from adapting to quickly deep in training process. I noticed that starting with a higher learning rate speeds up the earlier training quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_architecture(x, mu=0, sigma=0.1):\n",
    "\n",
    "    # Layer 1\n",
    "    CONVOL1_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 1, 8), mean = mu, stddev = sigma))\n",
    "    CONVOL1_b = tf.Variable(tf.zeros(8))\n",
    "    CONVOL1   = tf.nn.conv2d(x, CONVOL1_W, strides=[1, 1, 1, 1], padding='SAME') + CONVOL1_b\n",
    "    CONVOL1 = tf.nn.relu(CONVOL1)\n",
    "    CONVOL1 = tf.nn.dropout(CONVOL1, 0.95)\n",
    "    \n",
    "    # Layer 2\n",
    "    CONVOL2_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 8, 16), mean = mu, stddev = sigma))\n",
    "    CONVOL2_b = tf.Variable(tf.zeros(16))\n",
    "    CONVOL2   = tf.nn.conv2d(CONVOL1, CONVOL2_W, strides=[1, 1, 1, 1], padding='VALID') + CONVOL2_b\n",
    "    CONVOL2 = tf.nn.relu(CONVOL2)\n",
    "    CONVOL2 = tf.nn.max_pool(CONVOL2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    CONVOL2 = tf.nn.dropout(CONVOL2, 0.9)\n",
    "                           \n",
    "    # Layer 3\n",
    "    CONVOL3_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 16, 32), mean = mu, stddev = sigma))\n",
    "    CONVOL3_b = tf.Variable(tf.zeros(32))\n",
    "    CONVOL3   = tf.nn.conv2d(CONVOL2, CONVOL3_W, strides=[1, 1, 1, 1], padding='VALID') + CONVOL3_b\n",
    "    CONVOL3 = tf.nn.relu(CONVOL3)\n",
    "    CONVOL3 = tf.nn.max_pool(CONVOL3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    CONVOL3 = tf.nn.dropout(CONVOL3, 0.9)\n",
    "    \n",
    "    # Layer 4       \n",
    "    DENSE0  = tf.contrib.layers.flatten(CONVOL3)\n",
    "    DENSE1_W  = tf.Variable(tf.truncated_normal(shape=(1152, 512), mean = mu, stddev = sigma))\n",
    "    DENSE1_b  = tf.Variable(tf.zeros(512))\n",
    "    DENSE1    = tf.matmul(DENSE0, DENSE1_W) + DENSE1_b\n",
    "    DENSE1    = tf.nn.relu(DENSE1)\n",
    "    DENSE1    = tf.nn.dropout(DENSE1, 0.6)\n",
    "    \n",
    "    # Later 5\n",
    "    DENSE2_W  = tf.Variable(tf.truncated_normal(shape=(512, 256), mean = mu, stddev = sigma))\n",
    "    DENSE2_b  = tf.Variable(tf.zeros(256))\n",
    "    DENSE2    = tf.matmul(DENSE1, DENSE2_W) + DENSE2_b\n",
    "    DENSE2    = tf.nn.relu(DENSE2)\n",
    "    DENSE2    = tf.nn.dropout(DENSE2, 0.6)\n",
    "    \n",
    "    # Layer 6\n",
    "    DENSE3_W  = tf.Variable(tf.truncated_normal(shape=(256, 128), mean = mu, stddev = sigma))\n",
    "    DENSE3_b  = tf.Variable(tf.zeros(128))\n",
    "    DENSE3 = tf.matmul(DENSE2, DENSE3_W) + DENSE3_b\n",
    "    DENSE3 = tf.nn.relu(DENSE3)\n",
    "    DENSE3 = tf.nn.dropout(DENSE3, 0.6)\n",
    "    \n",
    "    # Layer 7\n",
    "    DENSE4_W  = tf.Variable(tf.truncated_normal(shape=(128, 43), mean = mu, stddev = sigma))\n",
    "    DENSE4_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(DENSE3, DENSE4_W) + DENSE4_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 1e-3\n",
    "decay = 1e-5*2\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32,32,1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y,43)\n",
    "\n",
    "logits = CNN_architecture(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For monitoring the training training and validation accuracy will be stored and printed during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_acc = []\n",
    "v_acc = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        # shuffle\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        \n",
    "        train_accuracy = evaluate(X_train, y_train)\n",
    "        t_acc.append(round(train_accuracy,3))\n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        v_acc.append(round(validation_accuracy,3))\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Training Accuracy   = {:.3f}\".format(train_accuracy))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "        #reduce rate\n",
    "        rate -= decay\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "\n",
    "        \n",
    "    saver.save(sess, './cnn_model')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With training done, I can now plot the training and validation accuary over Epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_acc, label='Training Accuracy')\n",
    "plt.plot(v_acc, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final rundown on the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    train_accuracy = evaluate(X_train, y_train)\n",
    "    print(\"Train Accuracy = {:.3f}\".format(train_accuracy))\n",
    "    \n",
    "    valid_accuracy = evaluate(X_valid, y_valid)\n",
    "    print(\"Valid Accuracy = {:.3f}\".format(valid_accuracy))    \n",
    "    \n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To highlight the performance of the model I searched for some traffic sign images and formated them according to the model input (32x32x3). The target class was then taken from the appending signnames list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "signnames = pd.read_csv('signnames.csv')\n",
    "signnames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_imgs = [mpimg.imread(f'custom_data/{i}.jpg') for i in range(1,6)]\n",
    "custom_imgs_copy = [img.copy() for img in custom_imgs]\n",
    "    \n",
    "labels = [\n",
    "    '28',\n",
    "    '40',\n",
    "    '14',\n",
    "    '13',\n",
    "    '1'\n",
    "]\n",
    "\n",
    "print('Custom sample images with Label')\n",
    "\n",
    "fig, ax = plt.subplots(1,5, dpi=160, figsize=(5,5))\n",
    "for i in range(5):\n",
    "    ax[i].axis('off')\n",
    "    cv2.putText(custom_imgs[i], labels[i],(15, 10),cv2.FONT_HERSHEY_SIMPLEX,0.4,(0,255,0),1)\n",
    "    ax[i].imshow(custom_imgs[i])    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_imgs_edit = [scale(contrast_equalization(i)[:,:,0]) for i in custom_imgs_copy]\n",
    "\n",
    "print('Final input for CNN')\n",
    "\n",
    "fig, ax = plt.subplots(1,5, dpi=160, figsize=(5,5))\n",
    "for i in range(5):\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(custom_imgs_edit[i], cmap='gray')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_top_5 = []\n",
    "\n",
    "for i in range(5):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, \"./cnn_model\")\n",
    "        top_5 = sess.run(tf.nn.top_k(tf.nn.softmax(logits), k=5), feed_dict={x: [custom_imgs_edit[i].reshape(32, 32, 1)]})\n",
    "        imgs_top_5.append([\n",
    "            custom_imgs_edit[i],\n",
    "            top_5\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, dpi=160, figsize=(7,7))\n",
    "for i in range(5):\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(custom_imgs[i])    \n",
    "\n",
    "fig, ax = plt.subplots(1,5, dpi=160, figsize=(7,7))\n",
    "for i in range(5):\n",
    "    ax[i].barh(np.array(imgs_top_5[i][1].indices[0], str), imgs_top_5[i][1].values[0])\n",
    "    asp = np.diff(ax[i].get_xlim())[0] / np.diff(ax[i].get_ylim())[0]\n",
    "    ax[i].set_aspect(asp)\n",
    "    ax[i].set_xlabel('Prediction', fontsize = 5.0)\n",
    "    for tick in ax[i].xaxis.get_major_ticks():\n",
    "                tick.label.set_fontsize(5) \n",
    "    for tick in ax[i].yaxis.get_major_ticks():\n",
    "                tick.label.set_fontsize(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the model identified 80% of the custom images correctly with a high confidence window on all but the wrongly identified one. I was actually surprised that it was mistaken for a 60 km/h sign, because obviously its not round. In the original data this sign was not one of the ones that occured that often, so maybe its features were not extracted well in the data augmentation process. However the roundabout sign was also pretty underrepresented in the training data and was no challenge at all for the model. Maybe its triangular shape was not detected due to the perspective of the image. \n",
    "\n",
    "I believe some improvements could still be made in data processing, as well as data augmentation. Those were pretty weak considering real life aquisition of the data. \n",
    "\n",
    "Also I believe the CNN programming done is not really state of the art. Initially I used the Keras environment within a newer implementation of Tensorflow on my local machine and got better results faster with a smaller neural net with much less code. The accuracy of Keras implementation was around 97% and featured much less code, due to all of the custom function and pipeline are implemented from scratch. I was not able to reproduce the same because I am not that familiar with this older Tensorflow version, but gladly I barely passed the required 93% accuracy :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keras Classifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.layers import CONVOL3D, MaxPooling2D\n",
    "\n",
    "class CNN_model:\n",
    "\n",
    "\tdef __init__(x_shape, y_shape):\n",
    "\n",
    "\t\tself.model = Sequential()\n",
    "\n",
    "\t\t# Layer 1\n",
    "\t\tself.model.add(CONVOL3D(256,(3,3), activation='relu', padding='same', input_shape=x_shape))\n",
    "\t\tself.model.add(MaxPooling2D(pool_size(2,2)))\n",
    "\t\tself.model.add(Dropout(0.2))\n",
    "\n",
    "\t\t# Layer 2\n",
    "\t\tself.model.add(CONVOL3D(256,(3,3), activation='relu', padding='same'))\n",
    "\t\tself.model.add(MaxPooling2D(pool_size(2,2)))\n",
    "\t\tself.model.add(Dropout(0.2))\n",
    "\n",
    "\t\t# Layer 3\n",
    "\t\tself.model.add(Flatten())\n",
    "\t\tself.model.add(Dense(256, activation='relu'))\n",
    "\n",
    "\t\t# Layer 4\n",
    "\t\tself.model.add(Dense(y_shape, activation='softmax'))\n",
    "\n",
    "\n",
    "\t\t# Model compile\n",
    "\t\tself.opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "\t\tself.model.compile(loss='sparse_categorical_crossentropy',\n",
    "\t\t              optimizer=self.opt,\n",
    "\t\t              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\tdef fit_model(self, x, y, x_validation, y_validation, BATCH_SIZE = 256, EPOCHS=20):\n",
    "\n",
    "\t\tself.model.fit(x,y, \n",
    "\t\t\tbatch_size=BATCH_SIZE, \n",
    "\t\t\tepochs=EPOCHS,\n",
    "\t\t\tvalidation_data=(x_validation, y_validation))\n",
    "\n",
    "\tdef evaluate_model(self):\n",
    "\n",
    "\t\tplt.plot(history.history['accuracy'], label='accuracy')\n",
    "\t\tplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "\t\tplt.xlabel('Epoch')\n",
    "\t\tplt.ylabel('Accuracy')\n",
    "\t\tplt.ylim([0.5, 1])\n",
    "\t\tplt.legend(loc='lower right')\n",
    "\n",
    "\t\ttest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
